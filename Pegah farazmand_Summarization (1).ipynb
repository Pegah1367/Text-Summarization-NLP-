{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mx8iuowuaDYj"
   },
   "source": [
    "# Summarization\n",
    "## This notebook outlines the concepts behind Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZL3aN78NaDYj"
   },
   "source": [
    "## Summarization\n",
    "- concept of capturing very important gist of a long piece of text\n",
    "\n",
    "### Types of Summarization\n",
    "- 1. **Extractive Summarization**\n",
    "    - Select sentences from the corpus that best represent the text\n",
    "    - Arrange them to form a summary\n",
    "- 2. **Abstractive Summarization**\n",
    "    - Captures the very important sentences from the text\n",
    "    - Paraphrases them to form a summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8B9-4i19aDYk"
   },
   "source": [
    "## Summarization Libraries\n",
    "- Sumy\n",
    "- Gensim\n",
    "- Summa\n",
    "- BERT **\n",
    "    - BART **\n",
    "    - PEGASUS **\n",
    "    - T5 **\n",
    "\n",
    "** Will be seen in DL-1\n",
    "\n",
    "\n",
    "## 1. Sumy :\n",
    "    1. Luhn – Heurestic method\n",
    "    2. Latent Semantic Analysis\n",
    "    4. LexRank – Unsupervised approach inspired by algorithms PageRank and HITS\n",
    "    5. TextRank - Graph-based summarization technique with keyword extractions in from document\n",
    "\n",
    "Documentation Reference [sumy](https://github.com/miso-belica/sumy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfOxJ-z5aDYk"
   },
   "source": [
    "## Task: Take a piece of text from wiki page and summarize them using Sumy\n",
    "### Steps\n",
    "- Install the necessary libraries\n",
    "- Import the libraries\n",
    "- Scrape the text from a pre-defined webpage\n",
    "- Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzMnyacRaDYk"
   },
   "source": [
    "### Install Sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q1ipX3XMaDYk",
    "outputId": "fbe2f9bb-3418-42b6-89df-80cfb7022cd9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sumy in c:\\users\\pegah\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: docopt<0.7,>=0.6.1 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from sumy) (0.6.2)\n",
      "Requirement already satisfied: breadability>=0.1.20 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from sumy) (0.1.20)\n",
      "Requirement already satisfied: requests>=2.7.0 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from sumy) (2.32.3)\n",
      "Requirement already satisfied: pycountry>=18.2.23 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from sumy) (24.6.1)\n",
      "Requirement already satisfied: nltk>=3.0.2 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from sumy) (3.9.1)\n",
      "Requirement already satisfied: chardet in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
      "Requirement already satisfied: lxml>=2.0 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from breadability>=0.1.20->sumy) (5.3.0)\n",
      "Requirement already satisfied: click in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (2025.6.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from click->nltk>=3.0.2->sumy) (0.4.6)\n",
      "Requirement already satisfied: lxml_html_clean in c:\\users\\pegah\\anaconda3\\lib\\site-packages (0.4.2)\n",
      "Requirement already satisfied: lxml in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from lxml_html_clean) (5.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install sumy\n",
    "!pip install lxml_html_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0Th3-l_aDYl"
   },
   "source": [
    "### Import the libraries\n",
    "- HtmlParser\n",
    "- Tokenizer\n",
    "- TextRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DciqM_GNaDYl"
   },
   "outputs": [],
   "source": [
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgiABTwhaDYl"
   },
   "source": [
    "### Scrape the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "A0-cIPliaDYl"
   },
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Automatic_summarization\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eYPtnnjsaDYl",
    "outputId": "a835c1b5-0597-42b0-b11c-7d9a19dd9d0d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package english_wordnet to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package english_wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package mock_corpus to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mock_corpus is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\pegah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoM17XQqaDYl"
   },
   "source": [
    "### Summarize - TextRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TGFE7dlYaDYl",
    "outputId": "7e18d84e-6269-4f93-e208-eacbe812463b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.\n",
      "Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm[16] exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages.\n",
      "Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).\n",
      "While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.\n",
      "A Class of Submodular Functions for Document Summarization\", The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), 2011^ Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, Learning Mixtures of Submodular Functions for Image Collection Summarization, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.^ Ramakrishna Bairi, Rishabh Iyer, Ganesh Ramakrishnan and Jeff Bilmes, Summarizing Multi-Document Topic Hierarchies using Submodular Mixtures, To Appear In the Annual Meeting of the Association for Computational Linguistics (ACL), Beijing, China, July - 2015^ Kai Wei, Rishabh Iyer, and Jeff Bilmes, Submodularity in Data Subset Selection and Active Learning Archived 2017-03-13 at the Wayback Machine, To Appear In Proc.\n"
     ]
    }
   ],
   "source": [
    "parser = HtmlParser.from_url(url, Tokenizer(\"english\"))\n",
    "summarizer = TextRankSummarizer()\n",
    "summary = summarizer(parser.document, 5)\n",
    "\n",
    "for sentence in summary:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5-2nd3kaDYm",
    "outputId": "d4c3c0eb-f087-4287-ec4c-28aa4714a683"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQZ4MAbxaDYm"
   },
   "source": [
    "### Try different Summarizers\n",
    "- LexRankSummarizer\n",
    "- LuhnSummarizer\n",
    "- LsaSummarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaFG4qq0aDYm"
   },
   "source": [
    "### Import the summarizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hLCkwC9_aDYm"
   },
   "outputs": [],
   "source": [
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29wAdL-2aDYm"
   },
   "source": [
    "### Create Summarizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yUsWZBKaDYm"
   },
   "source": [
    "### LexRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3v0FRE78aDYm",
    "outputId": "c1ec5430-0631-4358-f0e4-2cdafcd548f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document.\n",
      "The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".\n",
      "The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training.\n",
      "For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document.\n",
      "Automatic Text Summarization.\n"
     ]
    }
   ],
   "source": [
    "summarizer = LexRankSummarizer()\n",
    "summary = summarizer(parser.document, 5)\n",
    "for sentence in summary:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5CrXabWaDYm"
   },
   "source": [
    "### LuhnSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwOOPz9kaDYm",
    "outputId": "e03c06ac-b12d-4085-aa06-606300cecb03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge.\n",
      "Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).\n",
      "For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together.\n",
      "It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system ( MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights.\n",
      "A Class of Submodular Functions for Document Summarization\", The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), 2011^ Sebastian Tschiatschek, Rishabh Iyer, Hoachen Wei and Jeff Bilmes, Learning Mixtures of Submodular Functions for Image Collection Summarization, In Advances of Neural Information Processing Systems (NIPS), Montreal, Canada, December - 2014.^ Ramakrishna Bairi, Rishabh Iyer, Ganesh Ramakrishnan and Jeff Bilmes, Summarizing Multi-Document Topic Hierarchies using Submodular Mixtures, To Appear In the Annual Meeting of the Association for Computational Linguistics (ACL), Beijing, China, July - 2015^ Kai Wei, Rishabh Iyer, and Jeff Bilmes, Submodularity in Data Subset Selection and Active Learning Archived 2017-03-13 at the Wayback Machine, To Appear In Proc.\n"
     ]
    }
   ],
   "source": [
    "summarizer = LuhnSummarizer()\n",
    "summary = summarizer(parser.document, 5)\n",
    "for sentence in summary:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7JYXFhFQaDYm"
   },
   "source": [
    "### LsaSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tN2KTq-JaDYm",
    "outputId": "8d0c3718-2b5d-47cd-a7e9-0e9db9420ecb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases.\n",
      "Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney's seminal paper.\n",
      "It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system ( MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights.\n",
      "Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.\n",
      "Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity.\n"
     ]
    }
   ],
   "source": [
    "summarizer = LsaSummarizer()\n",
    "summary = summarizer(parser.document, 5)\n",
    "for sentence in summary:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2BgHn8raDYm"
   },
   "source": [
    "## 2. Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHe79VdGaDYm"
   },
   "source": [
    "## Task: Take a piece of text from wiki page and summarize them using Gensim\n",
    "### Steps\n",
    "- Install the necessary libraries\n",
    "- Import the libraries\n",
    "- Scrape the text from a pre-defined webpage\n",
    "- Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRuGGWalaDYm"
   },
   "source": [
    "### Install the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4m344wkOaDYm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim==3.8.0\n",
      "  Downloading gensim-3.8.0.tar.gz (23.4 MB)\n",
      "     ---------------------------------------- 0.0/23.4 MB ? eta -:--:--\n",
      "     ------------ --------------------------- 7.3/23.4 MB 45.4 MB/s eta 0:00:01\n",
      "     ------------------- ------------------- 11.5/23.4 MB 35.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 21.0/23.4 MB 35.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  23.3/23.4 MB 29.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 23.4/23.4 MB 27.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from gensim==3.8.0) (2.1.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from gensim==3.8.0) (1.15.3)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from gensim==3.8.0) (1.17.0)\n",
      "Collecting smart_open>=1.7.0 (from gensim==3.8.0)\n",
      "  Using cached smart_open-7.3.0.post1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from smart_open>=1.7.0->gensim==3.8.0) (1.17.0)\n",
      "Using cached smart_open-7.3.0.post1-py3-none-any.whl (61 kB)\n",
      "Building wheels for collected packages: gensim\n",
      "  Building wheel for gensim (setup.py): started\n",
      "  Building wheel for gensim (setup.py): finished with status 'done'\n",
      "  Created wheel for gensim: filename=gensim-3.8.0-cp313-cp313-win_amd64.whl size=23718100 sha256=a0d574a5bbbf4678b0297a479d696ba9b9cea30ca2fb34f828625548b2780cd9\n",
      "  Stored in directory: c:\\users\\pegah\\appdata\\local\\pip\\cache\\wheels\\0d\\83\\5e\\c54b3b23f61f607f3a902b75ec4a40731a9b9cb6fd9b5b3b53\n",
      "Successfully built gensim\n",
      "Installing collected packages: smart_open, gensim\n",
      "\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   ---------------------------------------- 0/2 [smart_open]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   -------------------- ------------------- 1/2 [gensim]\n",
      "   ---------------------------------------- 2/2 [gensim]\n",
      "\n",
      "Successfully installed gensim-3.8.0 smart_open-7.3.0.post1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'gensim' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'gensim'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install gensim==3.8.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AdNIGjXQGN8a"
   },
   "outputs": [],
   "source": [
    "#Pay Attention Please"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxG5880DCRS0"
   },
   "source": [
    "I tried using the Gensim `summarize()` function, but as it's only available in version 3.8.3, I faced some challenges. First, I attempted the installation in Google Colab, but it failed due to compatibility issues and lack of support for custom environments.\n",
    "\n",
    "To go further, I installed Anaconda on my local Windows system and created a dedicated environment with Python 3.8 to install `gensim==3.8.3`. However, the installation failed again due to system-level errors related to missing Microsoft Visual C++ Build Tools, and also no matching pre-built binary was found even using the `--only-binary` option.\n",
    "\n",
    "At this point, I have tried both cloud and local approaches, but I was unable to install Gensim 3.8.3 successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8NNwWEVqBO_a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: requests in c:\\users\\pegah\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from requests) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_ZeQPjYaDYm"
   },
   "source": [
    "### Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Hk7bNl0JaDYm"
   },
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "299G-nipaDYm"
   },
   "source": [
    "### Scrape the text\n",
    "- Use beautifulSoup to extract text (from Task1 of ML-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d-vx1FsaDYm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QLFngw__aDYm"
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6gFW6ri9aDYm"
   },
   "outputs": [],
   "source": [
    "def collect_text(soup):\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = ' '.join([para.get_text() for para in paragraphs])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "E5zHOCgZaDYn"
   },
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Automatic_summarization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "vqKS1fsWaDYn",
    "outputId": "b211a8be-5d83-442f-d935-e9025fb1fe32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content.\n",
      "Text summarization is usually implemented by natural language processing methods, designed to locate the most informative sentences in a given document.[1] On the other hand, visual content can be summarized using computer vision algorithms.\n",
      "Image summarization is the subject of ongoing research; existing approaches typically attempt to display the most representative images from a given image collection, or generate a video that only includes the most important content from the entire collection.[2][3][4] Video summarization algorithms identify and extract from the original video content the most important frames (key-frames), and/or the most important video segments (key-shots), normally in a temporally ordered fashion.[5][6][7][8] Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of video synopsis algorithms, where new video frames are being synthesized based on the original video content.\n",
      "Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above.\n",
      "For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.[10] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).[11]\n",
      "Abstractive summarization methods generate new text that did not exist in the original text.[12] This has been applied mainly for text.\n",
      "Abstractive methods build an internal semantic representation of the original content (often called a language model), and then use this representation to create a summary that is closer to what a human might express.\n",
      "Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge.\n",
      "The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.).\n",
      "Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\n",
      "At a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set.\n",
      "These algorithms model notions like diversity, coverage, information and representativeness of the summary.\n",
      "Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function, Determinantal point process, maximal marginal relevance (MMR) etc.\n",
      "You are given a piece of text, such as a journal article, and you must produce a list of keywords or key[phrase]s that capture the primary topics discussed in the text.[14] In the case of research articles, many authors provide manually assigned keywords, but most text lacks pre-existing keyphrases.\n",
      "For example, news articles rarely have keyphrases attached, but it would be useful to be able to automatically do so for a number of applications discussed below.\n",
      "Beginning with the work of Turney,[15] many researchers have approached keyphrase extraction as a supervised machine learning problem.\n",
      "For example, if we use only unigrams, bigrams, and trigrams, then we will never be able to extract a known keyphrase containing four words.\n",
      "We also need to create features that describe the examples and are informative enough to allow a learning algorithm to discriminate keyphrases from non- keyphrases.\n",
      "Typically features involve various term frequencies (how many times a phrase appears in the current text or in a larger corpus), the length of the example, relative position of the first occurrence, various Boolean syntactic features (e.g., contains all caps), etc.\n",
      "Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney's seminal paper.\n",
      "Once examples and features are created, we need a way to learn to predict keyphrases.\n",
      "While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of training data.\n",
      "Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm[16] exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages.\n",
      "For keyphrase extraction, it builds a graph using some set of text units as vertices.\n",
      "For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together.\n",
      "Since this method simply ranks the individual vertices, we need a way to threshold or produce a limited number of keyphrases.\n",
      "Like keyphrase extraction, document summarization aims to identify the essence of a text.\n",
      "Supervised text summarization is very much like supervised keyphrase extraction.\n",
      "Basically, if you have a collection of documents and human-generated summaries for them, you can learn features of sentences that make them good candidates for inclusion in the summary.\n",
      "Features might include the position in the document (i.e., the first few sentences are probably important), the number of words in the sentence, etc.\n",
      "The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".\n",
      "The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training.\n",
      "During the DUC 2001 and 2002 evaluation workshops, TNO developed a sentence extraction system for multi-document summarization in the news domain.\n",
      "The unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data.\n",
      "Some unsupervised summarization approaches are based on finding a \"centroid\" sentence, which is the mean word vector of all the sentences in the document.\n",
      "LexRank[19] is an algorithm essentially identical to TextRank, and both use this approach for document summarization.\n",
      "The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.\n",
      "It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system (MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights.\n",
      "Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic.\n",
      "In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload.\n",
      "Multi-document extractive summarization faces a problem of redundancy.\n",
      "The algorithm is called GRASSHOPPER.[22] In addition to explicitly promoting diversity during the ranking process, GRASSHOPPER incorporates a prior ranking (based on sentence position in the case of summarization).\n",
      "The state of the art results for multi-document summarization are obtained using mixtures of submodular functions.\n",
      "These methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07.[23] Similar results were achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04.[24]\n",
      "A new method for multi-lingual multi-document summarization that avoids redundancy generates ideograms to represent the meaning of each sentence in each document, then evaluates similarity by comparing ideogram shape and position.\n",
      "The idea of a submodular set function has recently emerged as a powerful modeling tool for various summarization problems.\n",
      "For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document.\n",
      "For example, work by Lin and Bilmes, 2012[26] shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization.\n",
      "It is very common for summarization and translation systems in NIST's Document Understanding Conferences.[2] ROUGE is a recall-based measure of how well a summary covers the content of human-generated summaries known as references.\n",
      "Domain-independent summarization techniques apply sets of general features to identify information-rich text segments.\n",
      "Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity.\n"
     ]
    }
   ],
   "source": [
    "text = collect_text(get_page(url))\n",
    "text\n",
    "\n",
    "summary = summarize(text, ratio=0.2)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLV8TBe_aDYn"
   },
   "source": [
    "### Summarize\n",
    "- **word_count**: maximum amount of words we want in the summary\n",
    "- **ratio**: fraction of sentences in the original text should be returned as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbsHbkhcaDYn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlHxikL1aDYq"
   },
   "source": [
    "## 3. Summa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KInlU0NaDYq"
   },
   "source": [
    "## Task: Take a piece of text from wiki page and summarize them using Gensim\n",
    "### Steps\n",
    "- Install the necessary libraries\n",
    "- Import the libraries\n",
    "- Scrape the text from a pre-defined webpage\n",
    "- Summarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9UCpcAtaDYq"
   },
   "source": [
    "### Install the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: summa in c:\\users\\pegah\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: scipy>=0.19 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from summa) (1.15.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from scipy>=0.19->summa) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install summa --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8ilgApiaDYq",
    "outputId": "a023c200-97ba-420e-80ca-df0e7116e7a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: summa in c:\\users\\pegah\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: scipy>=0.19 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from summa) (1.15.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from scipy>=0.19->summa) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install summa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPdsdgJ_aDYq"
   },
   "source": [
    "### Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Yq1EH8YsaDYq"
   },
   "outputs": [],
   "source": [
    "from summa.summarizer import summarize\n",
    "import requests\n",
    "from summa.summarizer import summarize\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "D6DISRX7DzUJ"
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def collect_text(soup):\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = ' '.join([para.get_text() for para in paragraphs])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "JnFLXL32DiaK"
   },
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Automatic_summarization\"\n",
    "text = collect_text(get_page(url))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shoycckqaDYq"
   },
   "source": [
    "### Scrape the text\n",
    "- Use beautifulSoup to extract text (from Task1 of ML-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_ZBYeGxaDYq"
   },
   "source": [
    "### Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uKUazIZaaDYq",
    "outputId": "1ccba1a8-f29c-4b7a-cd78-fb9ffa5a9a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content.\n",
      "Text summarization is usually implemented by natural language processing methods, designed to locate the most informative sentences in a given document.[1] On the other hand, visual content can be summarized using computer vision algorithms.\n",
      "Image summarization is the subject of ongoing research; existing approaches typically attempt to display the most representative images from a given image collection, or generate a video that only includes the most important content from the entire collection.[2][3][4] Video summarization algorithms identify and extract from the original video content the most important frames (key-frames), and/or the most important video segments (key-shots), normally in a temporally ordered fashion.[5][6][7][8] Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of video synopsis algorithms, where new video frames are being synthesized based on the original video content.\n",
      "Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above.\n",
      "For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.[10] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).[11]\n",
      "Abstractive summarization methods generate new text that did not exist in the original text.[12] This has been applied mainly for text.\n",
      "Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge.\n",
      "The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.).\n",
      "Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\n",
      "An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document.\n",
      "This problem is called multi-document summarization.\n",
      "At a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set.\n",
      "Query based summarization techniques, additionally model for relevance of the summary with the query.\n",
      "Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function, Determinantal point process, maximal marginal relevance (MMR) etc.\n",
      "They can enable document browsing by providing a short summary, improve information retrieval (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a full-text search), and be employed in generating index entries for a large text corpus.\n",
      "Beginning with the work of Turney,[15] many researchers have approached keyphrase extraction as a supervised machine learning problem.\n",
      "For example, if we use only unigrams, bigrams, and trigrams, then we will never be able to extract a known keyphrase containing four words.\n",
      "We also need to create features that describe the examples and are informative enough to allow a learning algorithm to discriminate keyphrases from non- keyphrases.\n",
      "Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm[16] exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages.\n",
      "For keyphrase extraction, it builds a graph using some set of text units as vertices.\n",
      "For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together.\n",
      "Like keyphrase extraction, document summarization aims to identify the essence of a text.\n",
      "Supervised text summarization is very much like supervised keyphrase extraction.\n",
      "Basically, if you have a collection of documents and human-generated summaries for them, you can learn features of sentences that make them good candidates for inclusion in the summary.\n",
      "The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\".\n",
      "The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training.\n",
      "During the DUC 2001 and 2002 evaluation workshops, TNO developed a sentence extraction system for multi-document summarization in the news domain.\n",
      "A promising approach is adaptive document/text summarization.[17] It involves first recognizing the text genre and then applying summarization algorithms optimized for this genre.\n",
      "The unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data.\n",
      "Some unsupervised summarization approaches are based on finding a \"centroid\" sentence, which is the mean word vector of all the sentences in the document.\n",
      "LexRank[19] is an algorithm essentially identical to TextRank, and both use this approach for document summarization.\n",
      "The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.\n",
      "It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system (MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights.\n",
      "Unlike TextRank, LexRank has been applied to multi-document summarization.\n",
      "Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic.\n",
      "In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload.\n",
      "Multi-document summarization creates information reports that are both concise and comprehensive.\n",
      "Multi-document extractive summarization faces a problem of redundancy.\n",
      "The algorithm is called GRASSHOPPER.[22] In addition to explicitly promoting diversity during the ranking process, GRASSHOPPER incorporates a prior ranking (based on sentence position in the case of summarization).\n",
      "The state of the art results for multi-document summarization are obtained using mixtures of submodular functions.\n",
      "These methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07.[23] Similar results were achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04.[24]\n",
      "A new method for multi-lingual multi-document summarization that avoids redundancy generates ideograms to represent the meaning of each sentence in each document, then evaluates similarity by comparing ideogram shape and position.\n",
      "The idea of a submodular set function has recently emerged as a powerful modeling tool for various summarization problems.\n",
      "For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document.\n",
      "For example, work by Lin and Bilmes, 2012[26] shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization.\n",
      "Similarly, work by Lin and Bilmes, 2011,[27] shows that many existing systems for automatic summarization are instances of submodular functions.\n",
      "This was a breakthrough result establishing submodular functions as the right models for summarization problems.[citation needed]\n",
      "Similarly, Bairi et al., 2015[29] show the utility of submodular functions for summarizing multi-document topic hierarchies.\n",
      "It is very common for summarization and translation systems in NIST's Document Understanding Conferences.[2] ROUGE is a recall-based measure of how well a summary covers the content of human-generated summaries known as references.\n",
      "Domain-independent summarization techniques apply sets of general features to identify information-rich text segments.\n",
      "Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity.\n"
     ]
    }
   ],
   "source": [
    "summary = summarize(text, ratio=0.2)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ErdjuNZaDYq"
   },
   "source": [
    "## ASSIGNMENT: Take the same medium article (the one I wrote) we used for Task 1 of ML-1 and extract the text and summarize them using all the above methods and provide the best summary with a note saying why the chosen library is the best\n",
    "url = https://medium.com/@subashgandyer/papa-what-is-a-neural-network-c5e5cc427c7\n",
    "\n",
    "### Submit 2 files\n",
    "- (notebook) .ipynb\n",
    "- (summary) .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lEv5DEmYGD2S",
    "outputId": "a26ce9e6-82c3-4279-8688-44b6879b32da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sumy in c:\\users\\pegah\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: docopt<0.7,>=0.6.1 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from sumy) (0.6.2)\n",
      "Requirement already satisfied: breadability>=0.1.20 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from sumy) (0.1.20)\n",
      "Requirement already satisfied: requests>=2.7.0 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from sumy) (2.32.3)\n",
      "Requirement already satisfied: pycountry>=18.2.23 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from sumy) (24.6.1)\n",
      "Requirement already satisfied: nltk>=3.0.2 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from sumy) (3.9.1)\n",
      "Requirement already satisfied: chardet in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
      "Requirement already satisfied: lxml>=2.0 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from breadability>=0.1.20->sumy) (5.3.0)\n",
      "Requirement already satisfied: click in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from requests>=2.7.0->sumy) (2025.6.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from click->nltk>=3.0.2->sumy) (0.4.6)\n",
      "Requirement already satisfied: summa in c:\\users\\pegah\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: scipy>=0.19 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from summa) (1.15.3)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from scipy>=0.19->summa) (2.1.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: requests in c:\\users\\pegah\\anaconda3\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from requests) (2025.6.15)\n",
      "Requirement already satisfied: gensim==3.8.0 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from gensim==3.8.0) (2.1.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from gensim==3.8.0) (1.15.3)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from gensim==3.8.0) (1.17.0)\n",
      "Requirement already satisfied: smart-open>=1.7.0 in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from gensim==3.8.0) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\pegah\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim==3.8.0) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sumy\n",
    "!pip install summa\n",
    "!pip install beautifulsoup4\n",
    "!pip install requests\n",
    "!pip install gensim==3.8.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "9yl4KfmXGmNa"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from summa.summarizer import summarize as summa_summarize\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from gensim.summarization import summarize as gensim_summarize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "eGBmybjtGo-V"
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def collect_text(soup):\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = ' '.join([para.get_text() for para in paragraphs])\n",
    "    return text\n",
    "\n",
    "url = \"https://medium.com/@subashgandyer/papa-what-is-a-neural-network-c5e5cc427c7\"\n",
    "text = collect_text(get_page(url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "EoA2sK7mGwIq"
   },
   "outputs": [],
   "source": [
    "summa_summary = summa_summarize(text, ratio=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    gensim_summary = gensim_summarize(text, ratio=0.2)\n",
    "except ValueError:\n",
    "    gensim_summary = \"Gensim failed to generate summary. Possibly text is too short or not clean.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "SDw9z8InGy-a"
   },
   "outputs": [],
   "source": [
    "parser = HtmlParser.from_url(url, Tokenizer(\"english\"))\n",
    "\n",
    "# TextRank\n",
    "text_rank = TextRankSummarizer()\n",
    "text_rank_summary = '\\n'.join(str(sentence) for sentence in text_rank(parser.document, 5))\n",
    "\n",
    "# Luhn\n",
    "luhn = LuhnSummarizer()\n",
    "luhn_summary = '\\n'.join(str(sentence) for sentence in luhn(parser.document, 5))\n",
    "\n",
    "# LexRank\n",
    "lex = LexRankSummarizer()\n",
    "lex_summary = '\\n'.join(str(sentence) for sentence in lex(parser.document, 5))\n",
    "\n",
    "# LSA\n",
    "lsa = LsaSummarizer()\n",
    "lsa_summary = '\\n'.join(str(sentence) for sentence in lsa(parser.document, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "7P2jsob9G6JK"
   },
   "outputs": [],
   "source": [
    "summaries = {\n",
    "    \"Summa (TextRank)\": summa_summary,\n",
    "    \"Gensim\": gensim_summary,\n",
    "    \"Sumy - TextRank\": text_rank_summary,\n",
    "    \"Sumy - Luhn\": luhn_summary,\n",
    "    \"Sumy - LexRank\": lex_summary,\n",
    "    \"Sumy - LSA\": lsa_summary,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "8Snjag2mKVyK"
   },
   "outputs": [],
   "source": [
    "note = \"\"\"\n",
    "===== Final Conclusion =====\n",
    "\n",
    "Chosen Library: Summa (TextRank-based)\n",
    "\n",
    "Why?\n",
    "✓ Among all methods, Summa produced the most coherent, compact, and relevant summary.\n",
    "✓ It does not require tokenization or parsing manually.\n",
    "✓ It works directly with plain HTML text.\n",
    "✓ Output is fluent, with logically connected sentences.\n",
    "\n",
    "Use Case: Ideal for quick and reliable summarization of online articles with minimal setup.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "rXVRmqQeIYaT"
   },
   "outputs": [],
   "source": [
    "with open(\"summary_comparison.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for method, summary in summaries.items():\n",
    "        f.write(f\"===== {method} =====\\n\")\n",
    "        f.write(f\"#Sentences: {summary.count('.')}\\n\")\n",
    "        f.write(f\"#Words: {len(summary.split())}\\n\")\n",
    "        f.write(summary + \"\\n\\n\")\n",
    "    f.write(note)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (gensim_env)",
   "language": "python",
   "name": "gensim_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
